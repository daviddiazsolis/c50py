{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8339c214",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to C5.0 Decision Trees with `c50py`\n",
    "\n",
    "This tutorial provides a deep dive into **`c50py`**, a modern Python implementation of Quinlan's C5.0 algorithm.\n",
    "\n",
    "We will cover:\n",
    "1.  **Why C5.0?** Key advantages over standard CART trees (scikit-learn).\n",
    "2.  **Native Categorical Support**: How `c50py` handles high-cardinality features and **automatically merges categories**.\n",
    "3.  **Robustness**: Handling missing values without imputation.\n",
    "4.  **Interpretability**: Extracting and tracing rules.\n",
    "5.  **Boosting**: Improving performance with C5.0-style boosting.\n",
    "6.  **Benchmarking**: Comparing performance against scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158025e9",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, ensure `c50py` is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85765e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install c50py graphviz pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from c50py import C5Classifier, C5Regressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23af2e9",
   "metadata": {},
   "source": [
    "## 2. The Power of Native Categorical Support\n",
    "\n",
    "One of the strongest features of C5.0 is its ability to handle categorical variables **natively**. \n",
    "\n",
    "Standard CART implementations (like scikit-learn's) require One-Hot Encoding (OHE). For a feature with $K$ categories, OHE creates $K$ binary columns. This leads to:\n",
    "*   **Sparse Data**: Inefficient memory usage.\n",
    "*   **Deep Trees**: The tree must make many splits (Is it 'A'? No. Is it 'B'? No...) to isolate a group.\n",
    "*   **Loss of Context**: The relationship between categories is lost.\n",
    "\n",
    "**C5.0**, on the other hand, splits categorical features into **subsets**. A single node can split a feature like `Color` into `{Red, Blue}` vs `{Green, Yellow}`. This is much more powerful and interpretable.\n",
    "\n",
    "### Demonstration: High Cardinality Feature\n",
    "Let's create a synthetic dataset with a high-cardinality categorical feature to see this in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "# Feature 1: \"City\" (High Cardinality - 20 categories)\n",
    "cities = [f\"City_{i}\" for i in range(20)]\n",
    "# Feature 2: \"Age\" (Numeric)\n",
    "ages = np.random.randint(18, 70, size=n_samples)\n",
    "\n",
    "# Assign target based on groups of cities\n",
    "# Group A: City_0 to City_9 -> High probability of Class 1\n",
    "# Group B: City_10 to City_19 -> High probability of Class 0\n",
    "X_cat = np.random.choice(cities, size=n_samples)\n",
    "y = []\n",
    "for city, age in zip(X_cat, ages):\n",
    "    city_idx = int(city.split('_')[1])\n",
    "    prob = 0.8 if city_idx < 10 else 0.2\n",
    "    # Add some noise/interaction with age\n",
    "    if age > 50: prob += 0.1\n",
    "    y.append(1 if np.random.rand() < prob else 0)\n",
    "\n",
    "df_syn = pd.DataFrame({'City': X_cat, 'Age': ages})\n",
    "y_syn = np.array(y)\n",
    "\n",
    "print(\"Data Sample:\")\n",
    "print(df_syn.head())\n",
    "print(f\"Unique Cities: {df_syn['City'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62857f",
   "metadata": {},
   "source": [
    "### Training C5.0 with Native Categoricals\n",
    "\n",
    "We simply pass the dataframe. We can specify `categorical_features` indices or names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize C5.0\n",
    "# We tell it that 'City' is categorical.\n",
    "# infer_categorical=True can also detect object columns automatically.\n",
    "clf_c5 = C5Classifier(feature_names=list(df_syn.columns), categorical_features=[\"City\"])\n",
    "\n",
    "t0 = time.time()\n",
    "clf_c5.fit(df_syn.values, y_syn)\n",
    "print(f\"C5.0 Training Time: {time.time() - t0:.4f}s\")\n",
    "\n",
    "# Visualize the tree structure\n",
    "# Notice how it groups cities!\n",
    "clf_c5.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ef936",
   "metadata": {},
   "source": [
    "**Observation**: Look at the output above. You should see a split like:\n",
    "`if City in {City_0, City_1, ...}:`\n",
    "This single node captures the logic that would take *many* nodes in a standard CART tree.\n",
    "\n",
    "### Comparison with Scikit-Learn (One-Hot Encoding)\n",
    "Now let's see what scikit-learn does with this data. We must One-Hot Encode first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for sklearn\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_ohe = enc.fit_transform(df_syn[['City']])\n",
    "X_num = df_syn[['Age']].values\n",
    "X_sklearn = np.hstack([X_ohe, X_num])\n",
    "\n",
    "clf_sk = DecisionTreeClassifier(random_state=42, max_depth=5) # Limit depth to keep it readable\n",
    "clf_sk.fit(X_sklearn, y_syn)\n",
    "\n",
    "# Let's look at the depth and node count\n",
    "print(f\"Sklearn Tree Depth: {clf_sk.get_depth()}\")\n",
    "print(f\"Sklearn Node Count: {clf_sk.get_n_leaves()}\")\n",
    "\n",
    "# It's much harder to read this tree because 'City' is split into 20 binary features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c5040",
   "metadata": {},
   "source": [
    "## 3. Missing Value Handling\n",
    "\n",
    "Real-world data is messy. `c50py` handles missing values (`NaN` or `None`) natively using **fractional case propagation**, the same strategy as the original C5.0.\n",
    "\n",
    "*   **Training**: If a value is missing at a split, the instance is sent down **both** branches with a weight proportional to the probability of that branch.\n",
    "*   **Prediction**: The prediction is a weighted average of the results from both branches.\n",
    "\n",
    "This avoids the need for arbitrary imputation (like filling with mean/median) which can distort data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missing values\n",
    "df_missing = df_syn.copy()\n",
    "# Randomly drop 20% of 'Age'\n",
    "mask = np.random.rand(n_samples) < 0.2\n",
    "df_missing.loc[mask, 'Age'] = np.nan\n",
    "\n",
    "print(f\"Missing values in Age: {df_missing['Age'].isna().sum()}\")\n",
    "\n",
    "# C5.0 handles this automatically\n",
    "clf_miss = C5Classifier(feature_names=[\"City\", \"Age\"], categorical_features=[\"City\"])\n",
    "clf_miss.fit(df_missing.values, y_syn)\n",
    "\n",
    "print(\"Training successful with missing values!\")\n",
    "# We can even trace a prediction for a sample with missing data\n",
    "sample_missing = df_missing.iloc[np.where(mask)[0][0]]\n",
    "print(f\"Sample with missing Age: \\n{sample_missing}\")\n",
    "print(f\"Prediction: {clf_miss.predict([sample_missing.values])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23566e16",
   "metadata": {},
   "source": [
    "## 4. Interpretability: Rules and Graphviz\n",
    "\n",
    "Decision trees are loved for interpretability. `c50py` provides tools to make this even better.\n",
    "\n",
    "### Export to Graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c57593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# Export the tree we trained earlier\n",
    "dot_data = clf_c5.export_graphviz(feature_names=[\"City\", \"Age\"], class_names=[\"Class 0\", \"Class 1\"], format=\"dot\")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "# If running locally, you can use graph.render(\"tree\") to save a PDF/PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac6777",
   "metadata": {},
   "source": [
    "### Extracting Rules\n",
    "Sometimes a list of rules is easier to read than a diagram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = clf_c5.export_rules(feature_names=[\"City\", \"Age\"], class_names=[\"Class 0\", \"Class 1\"])\n",
    "for r in rules[:5]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b2133",
   "metadata": {},
   "source": [
    "### Rule Tracing\n",
    "You can ask the model *why* it made a specific prediction for a specific sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the first sample\n",
    "sample = df_syn.iloc[0].values\n",
    "trace = clf_c5.predict_rule([sample], feature_names=[\"City\", \"Age\"])\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Reasoning: {trace[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bec7a",
   "metadata": {},
   "source": [
    "## 5. Boosting\n",
    "\n",
    "C5.0 is famous for its boosting implementation (similar to Adaboost). You can enable this simply by setting `trials > 1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a boosted ensemble with 10 trees\n",
    "clf_boost = C5Classifier(trials=10, feature_names=[\"City\", \"Age\"], categorical_features=[\"City\"])\n",
    "clf_boost.fit(df_syn.values, y_syn)\n",
    "\n",
    "print(f\"Boosted Ensemble Size: {len(clf_boost.ensemble_)} trees\")\n",
    "print(f\"Accuracy: {clf_boost.score(df_syn.values, y_syn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec632b",
   "metadata": {},
   "source": [
    "## 6. Benchmark: Titanic Dataset\n",
    "\n",
    "Let's put it all together on a real dataset: Titanic. We will compare `c50py` vs `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da104353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic Data\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df_titanic = pd.read_csv(url)\n",
    "\n",
    "# Preprocessing\n",
    "df_titanic = df_titanic.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "df_titanic['Age'] = df_titanic['Age'].fillna(df_titanic['Age'].median()) # Fill numeric for sklearn\n",
    "df_titanic['Embarked'] = df_titanic['Embarked'].fillna(df_titanic['Embarked'].mode()[0])\n",
    "df_titanic = df_titanic.dropna()\n",
    "\n",
    "X = df_titanic.drop(columns=['Survived'])\n",
    "y = df_titanic['Survived']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Sklearn (Requires OHE) ---\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "numeric_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# Simple manual OHE for demonstration\n",
    "X_train_ohe = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "X_test_ohe = pd.get_dummies(X_test, columns=categorical_cols)\n",
    "# Align columns\n",
    "X_train_ohe, X_test_ohe = X_train_ohe.align(X_test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "clf_sk = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "t0 = time.time()\n",
    "clf_sk.fit(X_train_ohe, y_train)\n",
    "sk_time = time.time() - t0\n",
    "sk_acc = clf_sk.score(X_test_ohe, y_test)\n",
    "\n",
    "# --- C5.0 (Native) ---\n",
    "# We pass the original dataframe (numpy array of objects)\n",
    "# We need to specify which columns are categorical\n",
    "cat_features = ['Sex', 'Embarked']\n",
    "# Note: Pclass is numeric in sklearn but could be categorical. Let's keep it numeric for parity.\n",
    "\n",
    "clf_c5 = C5Classifier(feature_names=list(X.columns), categorical_features=cat_features)\n",
    "t0 = time.time()\n",
    "clf_c5.fit(X_train.values, y_train)\n",
    "c5_time = time.time() - t0\n",
    "c5_acc = clf_c5.score(X_test.values, y_test)\n",
    "\n",
    "print(\"--- Results ---\")\n",
    "print(f\"Sklearn (CART) | Accuracy: {sk_acc:.4f} | Time: {sk_time:.4f}s\")\n",
    "print(f\"c50py (C5.0)   | Accuracy: {c5_acc:.4f} | Time: {c5_time:.4f}s\")\n",
    "\n",
    "# Let's try Boosting\n",
    "clf_c5_boost = C5Classifier(trials=10, feature_names=list(X.columns), categorical_features=cat_features)\n",
    "clf_c5_boost.fit(X_train.values, y_train)\n",
    "c5_boost_acc = clf_c5_boost.score(X_test.values, y_test)\n",
    "print(f\"c50py (Boost)  | Accuracy: {c5_boost_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a3536",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "`c50py` offers a powerful alternative to standard decision trees in Python, bringing:\n",
    "1.  **Cleaner Trees**: Thanks to native categorical grouping.\n",
    "2.  **Robustness**: Built-in missing value handling.\n",
    "3.  **Performance**: Competitive accuracy, often superior with boosting.\n",
    "4.  **Insight**: Easy-to-read rules and graphs.\n",
    "\n",
    "Give it a try on your next tabular dataset!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
